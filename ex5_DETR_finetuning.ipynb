{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed530f9-e77f-4d7b-8bf7-7024a0f88c98",
   "metadata": {},
   "source": [
    "# Finetuning Object Detection with Transformers (DETR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410ba7e-0550-432d-86bf-d4e3ad27e474",
   "metadata": {},
   "source": [
    "## 1. Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a9928-4c29-4a72-a6c1-57631708553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "import detr.util.misc as utils\n",
    "from detr.datasets import build_dataset, get_coco_api_from_dataset\n",
    "from detr.engine import evaluate, train_one_epoch\n",
    "from detr.models import build_model\n",
    "import detr.datasets as datasets\n",
    "\n",
    "from opt import get_args_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e71ea0-e2a2-4431-a433-6f825067469c",
   "metadata": {},
   "source": [
    "## 2. Preliminaries for PASCAL VOC dataset and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8e99f-a292-481c-9fc4-2155eda6c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASCAL VOC classes\n",
    "CLASSES = [\n",
    "    'background',\n",
    "    'aeroplane',\n",
    "    'bicycle',\n",
    "    'bird',\n",
    "    'boat',\n",
    "    'bottle',\n",
    "    'bus',\n",
    "    'car',\n",
    "    'cat',\n",
    "    'chair',\n",
    "    'cow',\n",
    "    'diningtable',\n",
    "    'dog',\n",
    "    'horse',\n",
    "    'motorbike',\n",
    "    'person',\n",
    "    'pottedplant',\n",
    "    'sheep',\n",
    "    'sofa',\n",
    "    'train',\n",
    "    'tvmonitor',\n",
    "]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2eb653-2ec2-4801-a65d-5161483e5334",
   "metadata": {},
   "source": [
    "## 3. GIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c16a8-43e5-4778-a389-e4aae576a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(x, y):\n",
    "    return ((x - y)**2).sum() ** (1 / 2)\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes \n",
    "    box 1 : (1, 4) shaped pytorch tensors - sinlge GT bounding box\n",
    "    box 2 : (N, 4) shaped pytorch tensors - multiple predictions from network\n",
    "    \"\"\"\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,0]+box1[:,2], box1[:,1]+box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,0]+box2[:,2], box2[:,1]+box2[:,3]\n",
    "\n",
    "    ## intersection rectangle coordinate\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "\n",
    "    ## intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2-inter_rect_x1, min=0.)\\\n",
    "            * torch.clamp(inter_rect_y2-inter_rect_y1, min=0.)\n",
    "\n",
    "    ## calculate iou\n",
    "    area_1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    area_2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "    \n",
    "    union = (area_1 + area_2 - inter_area)\n",
    "    iou = inter_area / union\n",
    "\n",
    "    return iou, union\n",
    "\n",
    "def generalized_bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ## practice\n",
    "    iou, union = bbox_iou(box1, box2)\n",
    "\n",
    "    lt = torch.min(box1[:, None, :2], box2[:, :2])\n",
    "    rb = torch.max(box1[:, None, 2:] + box1[:, None, :2], box2[:, 2:] + box2[:, :2])\n",
    "    \n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (area - union) / area\n",
    "\n",
    "def draw_bb(img, boxes, color='r'):\n",
    "    colors = ['r', 'b']\n",
    "    fig,ax = plt.subplots(1)\n",
    "    for i, box in enumerate(boxes):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0, 0],box[0, 1]),\n",
    "            box[0, 2],\n",
    "            box[0, 3],\n",
    "            linewidth=3,\n",
    "            edgecolor=colors[i],\n",
    "            facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e703ff-c58e-4272-aa71-0461c9083402",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box = torch.Tensor([[8, 8, 16, 16]])\n",
    "\n",
    "boxes = [\n",
    "    torch.Tensor([[7, 11, 12, 14]]),\n",
    "    torch.Tensor([[6, 3, 17, 11]]),\n",
    "    torch.Tensor([[9, 6, 13, 8]]),\n",
    "]\n",
    "\n",
    "img = np.ones([32, 32, 3]) * 255\n",
    "img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "\n",
    "for box in boxes:\n",
    "    iou, union = bbox_iou(gt_box, box)\n",
    "    giou = generalized_bbox_iou(gt_box, box).sum()\n",
    "    loss = l2(gt_box, box).sum()\n",
    "    print(f'IoU: {float(iou):.2f} / loss: {float(loss):.2f} / GIoU: {float(giou):.2f} ')\n",
    "    draw_bb(img, [gt_box, box])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cf42b-c383-44fd-86f4-e8cdb7b7571a",
   "metadata": {},
   "source": [
    "## 4. Load PASCAL VOC Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60160930-33bf-4273-acd0-37f3b4a0feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "parser = get_args_parser()\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "voc_train = build_dataset(image_set='train', args=args)\n",
    "voc_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    voc_train, batch_size=1, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    voc_val, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "base_ds = get_coco_api_from_dataset(voc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21adb21-a0a6-43ff-9063-5630e8c05821",
   "metadata": {},
   "source": [
    "## 5. Finetuning pre-trained DETR on PASCAL VOC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5babfb6f-cdda-474c-ae04-d096f2d3c6a9",
   "metadata": {},
   "source": [
    "### 5-1. Define DETR and criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983fc61-6394-430b-aafe-50d1c850aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, criterion, postprocessors\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "\n",
    "pretrained_url = 'https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth'\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                pretrained_url, map_location='cpu', check_hash=True)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "print(\"DETR on COCO Dataset\")\n",
    "print(model.class_embed)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 21  #  class (20) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.class_embed.in_features\n",
    "\n",
    "# replace the pre-trained class_embed with a new one\n",
    "model.class_embed = nn.Linear(in_features, num_classes + 1)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"DETR on PASCAL VOC Dataset\")\n",
    "print(model.class_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948888aa-8f72-4128-b4f3-00e3c1baad2e",
   "metadata": {},
   "source": [
    "### 5-2. Define optimizier and lr scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f64b0-f778-4e33-930d-e9fc81f36e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_dicts, lr=args.lr, weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16d81f-19c2-431a-8d93-4096e13fbde6",
   "metadata": {},
   "source": [
    "### 5-3. Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd84e9c-c30c-4ebe-99a7-5223592672fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "output_dir = Path('logs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 100 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    test_stats, coco_evaluator = evaluate(\n",
    "        model, criterion, postprocessors, data_loader_val, base_ds, device, output_dir\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                 **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                 'epoch': epoch,\n",
    "                 'n_parameters': n_parameters}\n",
    "\n",
    "#     if output_dir and utils.is_main_process():\n",
    "#         with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "#             f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "#     # for evaluation logs\n",
    "#     if coco_evaluator is not None:\n",
    "#         (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "#         if \"bbox\" in coco_evaluator.coco_eval:\n",
    "#             filenames = ['latest.pth']\n",
    "#             if epoch % 50 == 0:\n",
    "#                 filenames.append(f'{epoch:03}.pth')\n",
    "#             for name in filenames:\n",
    "#                 torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "#                                    output_dir / \"eval\" / name)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52aa239-e1fa-477c-bb7d-7cfe9a2985ba",
   "metadata": {},
   "source": [
    "## 6. Visualize Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbebeb5-7da6-409b-a478-9ab1d8911660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "im = Image.open('./VOC2007/JPEGImages/000001.jpg')\n",
    "\n",
    "# mean-std normalize the input image (batch-size: 1)\n",
    "img = transform(im).unsqueeze(0)\n",
    "\n",
    "# propagate through the model\n",
    "model.eval().cpu()\n",
    "outputs = model(img)\n",
    "\n",
    "# keep only predictions with 0.01+ confidence\n",
    "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "keep = probas.max(-1).values > 0.1\n",
    "\n",
    "# convert boxes from [0; 1] to image scales\n",
    "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "\n",
    "plot_results(im, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59fdcf0-fa35-4663-ba7c-cefed4884e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed530f9-e77f-4d7b-8bf7-7024a0f88c98",
   "metadata": {},
   "source": [
    "# Finetuning Object Detection with Transformers (DETR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410ba7e-0550-432d-86bf-d4e3ad27e474",
   "metadata": {},
   "source": [
    "## 1. Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409a9928-4c29-4a72-a6c1-57631708553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "import detr.util.misc as utils\n",
    "from detr.datasets import build_dataset, get_coco_api_from_dataset\n",
    "from detr.engine import evaluate, train_one_epoch\n",
    "from detr.models import build_model\n",
    "import detr.datasets as datasets\n",
    "\n",
    "from opt import get_args_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e71ea0-e2a2-4431-a433-6f825067469c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preliminaries for PASCAL VOC dataset and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf8e99f-a292-481c-9fc4-2155eda6c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASCAL VOC classes\n",
    "CLASSES = [\n",
    "    'background',\n",
    "    'aeroplane',\n",
    "    'bicycle',\n",
    "    'bird',\n",
    "    'boat',\n",
    "    'bottle',\n",
    "    'bus',\n",
    "    'car',\n",
    "    'cat',\n",
    "    'chair',\n",
    "    'cow',\n",
    "    'diningtable',\n",
    "    'dog',\n",
    "    'horse',\n",
    "    'motorbike',\n",
    "    'person',\n",
    "    'pottedplant',\n",
    "    'sheep',\n",
    "    'sofa',\n",
    "    'train',\n",
    "    'tvmonitor',\n",
    "]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def draw_bb(img, boxes, color='r'):\n",
    "    colors = ['r', 'b']\n",
    "    fig,ax = plt.subplots(1)\n",
    "    for i, box in enumerate(boxes):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0, 0],box[0, 1]),\n",
    "            box[0, 2],\n",
    "            box[0, 3],\n",
    "            linewidth=3,\n",
    "            edgecolor=colors[i],\n",
    "            facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2eb653-2ec2-4801-a65d-5161483e5334",
   "metadata": {},
   "source": [
    "## 3. GIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43c16a8-43e5-4778-a389-e4aae576a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(x, y):\n",
    "    return ((x - y)**2).sum() ** (1 / 2)\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes \n",
    "    box 1 : (1, 4) shaped pytorch tensors - sinlge GT bounding box\n",
    "    box 2 : (N, 4) shaped pytorch tensors - multiple predictions from network\n",
    "    \"\"\"\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,0]+box1[:,2], box1[:,1]+box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,0]+box2[:,2], box2[:,1]+box2[:,3]\n",
    "\n",
    "    ## intersection rectangle coordinate\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "\n",
    "    ## intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2-inter_rect_x1, min=0.)\\\n",
    "            * torch.clamp(inter_rect_y2-inter_rect_y1, min=0.)\n",
    "\n",
    "    ## calculate iou\n",
    "    area_1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    area_2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "    \n",
    "    union = (area_1 + area_2 - inter_area)\n",
    "    iou = inter_area / union\n",
    "\n",
    "    return iou, union\n",
    "\n",
    "def generalized_bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ## practice\n",
    "    iou, union = bbox_iou(box1, box2)\n",
    "\n",
    "    lt = torch.min(box1[:, None, :2], box2[:, :2])\n",
    "    rb = torch.max(box1[:, None, 2:] + box1[:, None, :2], box2[:, 2:] + box2[:, :2])\n",
    "    \n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (area - union) / area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e703ff-c58e-4272-aa71-0461c9083402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.25 / loss: 7.42 / GIoU: 0.19 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.28 / loss: 8.83 / GIoU: 0.26 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAL5klEQVR4nO3db6hcdX7H8fenMdKyCtVmlBC12RVZKks3yiUIlmVb65LmifpgYX2w5IGQfbCCwvZB2EJrn9lSXfqgCLGGDcW6CCpKkXZDsMjCYr3aGJNmu3El3Y0JybWyaJ90q377YE7gJr1/JjNnZm79vV8wzDm/OXN/X37czz1nzpz7O6kqJH32/dq8C5A0G4ZdaoRhlxph2KVGGHapEYZdasQVk7w5yS7gr4FNwN9W1aNrbb9ly5bavn37JF1KWsOpU6d4//33s9JrY4c9ySbgb4C7gdPA60leqqp/W+0927dvZ3FxcdwuJa1jYWFh1dcmOYzfCbxTVe9W1a+AHwD3TPDzJE3RJGHfBvxi2frprk3SBjRJ2Ff6XPB/rr1NsjfJYpLFpaWlCbqTNIlJwn4auHHZ+g3AmUs3qqr9VbVQVQuDwWCC7iRNYpKwvw7ckuTzSa4EvgG81E9Zkvo29tn4qvo4yYPAPzH86u1AVR3vrTJJvZroe/aqehl4uadaJE2RV9BJjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjZjojjBJTgEfAZ8AH1fV6neClzRXE4W98/tV9X4PP0fSFHkYLzVi0rAX8MMkbyTZ20dBkqZj0sP4O6vqTJLrgENJflJVry7foPsjsBfgpptumrA7SeOaaM9eVWe65/PAC8DOFbbZX1ULVbUwGAwm6U7SBMYOe5LPJbn6wjLwNeBYX4VJ6tckh/HXAy8kufBz/r6q/rGXqiT1buywV9W7wJd7rEXSFPnVm9SIPi6q0YiGn3g0C1XzrmDjcc8uNcKwS43wMH5OPMzsnx+T1uaeXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEV9D1bc3LuJZdNuflXlMw4fh+xi9rdM8uNcKwS43wMH6aLj0szBqvaXLjjG9DH6fcs0uNMOxSIwy71AjDLjXCsEuNMOxSIwy71Ih1w57kQJLzSY4ta7s2yaEkJ7vna6ZbpqRJjbJn/z6w65K2fcDhqroFONytS9rA1g17d7/1Dy5pvgc42C0fBO7ttyxJfRv3M/v1VXUWoHu+rr+SJE3D1E/QJdmbZDHJ4tLS0rS7k7SKccN+LslWgO75/GobVtX+qlqoqoXBYDBmd5ImNW7YXwL2dMt7gBf7KUfStIzy1dszwI+BLyY5neQB4FHg7iQngbu7dUkb2Lr/z15V96/y0l091yJpiryCTmqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWrEurPLajqSeVeg1rhnlxph2KVGeBg/Q1XzrkAtG+X2TweSnE9ybFnbI0neS3Kke+yebpmSJjXKYfz3gV0rtH+vqnZ0j5f7LUtS39YNe1W9Cnwwg1okTdEkJ+geTHK0O8y/preKJE3FuGF/ArgZ2AGcBR5bbcMke5MsJllcWloasztJkxor7FV1rqo+qapPgSeBnWtsu7+qFqpqYTAYjFunpAmNFfYkW5et3gccW21bSRvDut+zJ3kG+CqwJclp4M+ArybZARRwCvjW9EqU1Id1w15V96/Q/NQUapE0RV4uKzXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjVi3bAnuTHJK0lOJDme5KGu/dokh5Kc7J69bbO0gY2yZ/8Y+E5V/Q5wB/DtJLcC+4DDVXULcLhbl7RBrRv2qjpbVW92yx8BJ4BtwD3AwW6zg8C9U6pRUg8u6zN7ku3AbcBrwPVVdRaGfxCA63qvTlJvRg57kquA54CHq+rDy3jf3iSLSRaXlpbGqVFSD0YKe5LNDIP+dFU93zWfS7K1e30rcH6l91bV/qpaqKqFwWDQR82SxjDK2fgwvB/7iap6fNlLLwF7uuU9wIv9lyepL1eMsM2dwDeBt5Mc6dq+CzwKPJvkAeDnwNenUqGkXqwb9qr6EZBVXr6r33IkTYtX0EmNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNGGUqaY0rq03KK82ee3apEYZdaoSH8X2rmncF0opGudfbjUleSXIiyfEkD3XtjyR5L8mR7rF7+uVKGtcoe/aPge9U1ZtJrgbeSHKoe+17VfVX0ytPUl9GudfbWeBst/xRkhPAtmkXJqlfl3WCLsl24Dbgta7pwSRHkxxIck3fxUnqz8hhT3IV8BzwcFV9CDwB3AzsYLjnf2yV9+1NsphkcWlpafKKJY1lpLAn2cww6E9X1fMAVXWuqj6pqk+BJ4GdK723qvZX1UJVLQwGg77qlnSZRjkbH+Ap4ERVPb6sfeuyze4DjvVfnqS+jHI2/k7gm8DbSY50bd8F7k+yAyjgFPCtKdQnqSejnI3/EbDSRd4v91+OpGnxclmpEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEaPc6+3Xk/xLkreSHE/y5137tUkOJTnZPXvLZmkDG2XP/t/AH1TVlxnennlXkjuAfcDhqroFONytS9qg1g17Df1Xt7q5exRwD3Cwaz8I3DuNAiX1Y9T7s2/q7uB6HjhUVa8B11fVWYDu+bqpVSlpYiOFvao+qaodwA3AziRfGrWDJHuTLCZZXFpaGrNMSZO6rLPxVfVL4J+BXcC5JFsBuufzq7xnf1UtVNXCYDCYrFpJYxvlbPwgyW92y78B/CHwE+AlYE+32R7gxSnVKKkHV4ywzVbgYJJNDP84PFtV/5Dkx8CzSR4Afg58fYp1SprQumGvqqPAbSu0/ydw1zSKktQ/r6CTGmHYpUYYdqkRhl1qhGGXGpGqml1nyRLwH93qFuD9mXW+Ouu4mHVc7P9bHb9dVStevTbTsF/UcbJYVQtz6dw6rKPBOjyMlxph2KVGzDPs++fY93LWcTHruNhnpo65fWaXNFsexkuNmEvYk+xK8u9J3kkyt7nrkpxK8naSI0kWZ9jvgSTnkxxb1jbzCTxXqeORJO91Y3Ikye4Z1HFjkleSnOgmNX2oa5/pmKxRx0zHZGqTvFbVTB/AJuBnwBeAK4G3gFtnXUdXyylgyxz6/QpwO3BsWdtfAvu65X3AX8ypjkeAP57xeGwFbu+WrwZ+Ctw66zFZo46ZjgkQ4KpueTPwGnDHpOMxjz37TuCdqnq3qn4F/IDh5JXNqKpXgQ8uaZ75BJ6r1DFzVXW2qt7slj8CTgDbmPGYrFHHTNVQ75O8ziPs24BfLFs/zRwGtFPAD5O8kWTvnGq4YCNN4PlgkqPdYf5M7weQZDvD+RPmOqnpJXXAjMdkGpO8ziPsWaFtXl8J3FlVtwN/BHw7yVfmVMdG8gRwM8N7BJwFHptVx0muAp4DHq6qD2fV7wh1zHxMaoJJXlczj7CfBm5ctn4DcGYOdVBVZ7rn88ALDD9izMtIE3hOW1Wd637RPgWeZEZjkmQzw4A9XVXPd80zH5OV6pjXmHR9/5LLnOR1NfMI++vALUk+n+RK4BsMJ6+cqSSfS3L1hWXga8Cxtd81VRtiAs8Lv0yd+5jBmCQJ8BRwoqoeX/bSTMdktTpmPSZTm+R1VmcYLznbuJvhmc6fAX8ypxq+wPCbgLeA47OsA3iG4eHg/zA80nkA+C2Gt9E62T1fO6c6/g54Gzja/XJtnUEdv8fwo9xR4Ej32D3rMVmjjpmOCfC7wL92/R0D/rRrn2g8vIJOaoRX0EmNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXifwFoSQ8hum1pCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1\n",
    "\n",
    "gt_box = torch.Tensor([[8, 8, 16, 16]])\n",
    "\n",
    "boxes = [\n",
    "    torch.Tensor([[6, 3, 17, 11]]),\n",
    "    torch.Tensor([[9, 6, 13, 8]]),\n",
    "]\n",
    "\n",
    "img = np.ones([32, 32, 3]) * 255\n",
    "img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "\n",
    "for box in boxes:\n",
    "    iou, union = bbox_iou(gt_box, box)\n",
    "    giou = generalized_bbox_iou(gt_box, box).sum()\n",
    "    loss = l2(gt_box, box).sum()\n",
    "    print(f'IoU: {float(iou):.2f} / loss: {float(loss):.2f} / GIoU: {float(giou):.2f} ')\n",
    "    draw_bb(img, [gt_box, box])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f8a2b2-98b5-442d-a016-6d48c85f089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.00 / loss: 29.43 / GIoU: -0.38 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.00 / loss: 31.19 / GIoU: -0.46 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "gt_box = torch.Tensor([[8, 8, 16, 16]])\n",
    "\n",
    "boxes = [\n",
    "    torch.Tensor([[25, 25, 4, 4]]),\n",
    "    torch.Tensor([[27, 26, 4, 4]]),\n",
    "]\n",
    "\n",
    "img = np.ones([32, 32, 3]) * 255\n",
    "img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "\n",
    "for box in boxes:\n",
    "    iou, union = bbox_iou(gt_box, box)\n",
    "    giou = generalized_bbox_iou(gt_box, box).sum()\n",
    "    loss = l2(gt_box, box).sum()\n",
    "    print(f'IoU: {float(iou):.2f} / loss: {float(loss):.2f} / GIoU: {float(giou):.2f} ')\n",
    "    draw_bb(img, [gt_box, box])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cf42b-c383-44fd-86f4-e8cdb7b7571a",
   "metadata": {},
   "source": [
    "## 4. Load PASCAL VOC Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60160930-33bf-4273-acd0-37f3b4a0feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "parser = get_args_parser()\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "voc_train = build_dataset(image_set='train', args=args)\n",
    "voc_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    voc_train, batch_size=1, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    voc_val, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "base_ds = get_coco_api_from_dataset(voc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21adb21-a0a6-43ff-9063-5630e8c05821",
   "metadata": {},
   "source": [
    "## 5. Finetuning pre-trained DETR on PASCAL VOC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5babfb6f-cdda-474c-ae04-d096f2d3c6a9",
   "metadata": {},
   "source": [
    "### 5-1. Define DETR and criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983fc61-6394-430b-aafe-50d1c850aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, criterion, postprocessors\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "\n",
    "pretrained_url = 'https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth'\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                pretrained_url, map_location='cpu', check_hash=True)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "print(\"DETR on COCO Dataset\")\n",
    "print(model.class_embed)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 21  #  class (20) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.class_embed.in_features\n",
    "\n",
    "# replace the pre-trained class_embed with a new one\n",
    "model.class_embed = nn.Linear(in_features, num_classes + 1)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"DETR on PASCAL VOC Dataset\")\n",
    "print(model.class_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948888aa-8f72-4128-b4f3-00e3c1baad2e",
   "metadata": {},
   "source": [
    "### 5-2. Define optimizier and lr scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f64b0-f778-4e33-930d-e9fc81f36e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_dicts, lr=args.lr, weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16d81f-19c2-431a-8d93-4096e13fbde6",
   "metadata": {},
   "source": [
    "### 5-3. Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd84e9c-c30c-4ebe-99a7-5223592672fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "output_dir = Path('logs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 100 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    test_stats, coco_evaluator = evaluate(\n",
    "        model, criterion, postprocessors, data_loader_val, base_ds, device, output_dir\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                 **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                 'epoch': epoch,\n",
    "                 'n_parameters': n_parameters}\n",
    "\n",
    "#     if output_dir and utils.is_main_process():\n",
    "#         with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "#             f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "#     # for evaluation logs\n",
    "#     if coco_evaluator is not None:\n",
    "#         (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "#         if \"bbox\" in coco_evaluator.coco_eval:\n",
    "#             filenames = ['latest.pth']\n",
    "#             if epoch % 50 == 0:\n",
    "#                 filenames.append(f'{epoch:03}.pth')\n",
    "#             for name in filenames:\n",
    "#                 torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "#                                    output_dir / \"eval\" / name)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52aa239-e1fa-477c-bb7d-7cfe9a2985ba",
   "metadata": {},
   "source": [
    "## 6. Visualize Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbebeb5-7da6-409b-a478-9ab1d8911660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "im = Image.open('./VOC2007/JPEGImages/000001.jpg')\n",
    "\n",
    "# mean-std normalize the input image (batch-size: 1)\n",
    "img = transform(im).unsqueeze(0)\n",
    "\n",
    "# propagate through the model\n",
    "model.eval().cpu()\n",
    "outputs = model(img)\n",
    "\n",
    "# keep only predictions with 0.01+ confidence\n",
    "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "keep = probas.max(-1).values > 0.1\n",
    "\n",
    "# convert boxes from [0; 1] to image scales\n",
    "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "\n",
    "plot_results(im, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59fdcf0-fa35-4663-ba7c-cefed4884e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052bd907-50c6-416f-877b-23e3a9f2a031",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PASCAL VOC DataLoader & Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501e0cb-7ba6-4cdc-9d27-10f9dd78e605",
   "metadata": {},
   "source": [
    "## 1. Prepare PASCAL VOC2007 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e1b9a-6ded-42af-86fa-bbb81a7a6b32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## link: https://drive.google.com/file/d/1FSPPvm6-QZ43pCYzPA3-pyN7RZ0rq5DT/view?usp=sharing\n",
    "## id: 1FSPPvm6-QZ43pCYzPA3-pyN7RZ0rq5DT\n",
    "## filename: VOC2007.zip\n",
    "\n",
    "!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1FSPPvm6-QZ43pCYzPA3-pyN7RZ0rq5DT' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1FSPPvm6-QZ43pCYzPA3-pyN7RZ0rq5DT\" -O VOC2007.zip && rm -rf ~/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a732f89-3471-4a30-852f-32751ca8d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('VOC2007.zip', 'r')as f:\n",
    "    f.extractall('./')\n",
    "\n",
    "!rm -rf VOC2007.zip\n",
    "!rm -rf __MACOSX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f592c-2005-479f-9821-f4565dbfab91",
   "metadata": {},
   "source": [
    "## 2. Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44865d-05de-4545-bf34-7fb10130cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5405e-4802-4f67-9c1c-c927ce6050ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC2007_CLASSES = (\n",
    "    '__background__',\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor'\n",
    ")\n",
    "\n",
    "# Define bbox color\n",
    "VOC2007_COLORS = [\n",
    "    [0.,        0.,        0.       ],\n",
    " [0.5019608, 0.,        0.       ],\n",
    " [0.,        0.5019608, 0.       ],\n",
    " [0.5019608, 0.5019608, 0.       ],\n",
    " [0.,        0.,        0.5019608],\n",
    " [0.5019608, 0.,        0.5019608],\n",
    " [0.,        0.5019608, 0.5019608],\n",
    " [0.5019608, 0.5019608, 0.5019608],\n",
    " [0.2509804, 0.,        0.       ],\n",
    " [0.7529412, 0.,        0.       ],\n",
    " [0.2509804, 0.5019608, 0.       ],\n",
    " [0.7529412, 0.5019608, 0.       ],\n",
    " [0.2509804, 0.,        0.5019608],\n",
    " [0.7529412, 0.,        0.5019608],\n",
    " [0.2509804, 0.5019608, 0.5019608],\n",
    " [0.7529412, 0.5019608, 0.5019608],\n",
    " [0.,        0.2509804, 0.       ],\n",
    " [0.5019608, 0.2509804, 0.       ],\n",
    " [0.,        0.7529412, 0.       ],\n",
    " [0.5019608, 0.7529412, 0.       ],\n",
    " [0.,        0.2509804, 0.5019608]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2e2cf-90e9-4d6f-a953-e719c02473ed",
   "metadata": {},
   "source": [
    "## 3. Load an annotation file (working with a XML file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4021b-e9e5-498f-abff-734d720452f7",
   "metadata": {},
   "source": [
    "### xml file example\n",
    "- open any xml file by web browsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef88841-c99f-413c-ae53-713d129b4f98",
   "metadata": {},
   "source": [
    "- output of \"__getitems__\" should be\n",
    "\n",
    ">image : a PIL Image of size (H, W)  \n",
    ">target : a dict containing the following key  \n",
    ">>boxes (FloatTensor[N, 4]) :  the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H  \n",
    ">>labels (Int64Tensor[N]) : the label for each bounding box  \n",
    ">>image_id (Int64Tensor[1]): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation  \n",
    ">>area (Tensor[N]): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.  \n",
    ">>iscrowd (UInt8Tensor[N]): instances with iscrowd=True will be ignored during evaluation(all set zero) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbfc2c-2ac5-4512-a06f-9f8ea59a9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse xml file\n",
    "sample_idx = 1\n",
    "filename = os.path.join(f'./VOC2007/Annotations/{sample_idx:06d}.xml')\n",
    "tree = ET.parse(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64a02c-a71c-4a97-9a86-6465c6f8d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### explore xml file\n",
    "root = tree.getroot()\n",
    "print(\"root.tag:\", root.tag)\n",
    "for node in root:\n",
    "    print(\">>>>\", node.tag)\n",
    "print('\\n')\n",
    "\n",
    "print(\"node.tag:\", node.tag)\n",
    "for node2 in node:  ### iterate the final object\n",
    "    print(f\">>>> {node2.tag}: {node2.text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"node2.tag:\", node2.tag)\n",
    "for node3 in node2: ### iterate the final bndbox (bounding box)\n",
    "    print(f\">>>> {node3.tag}: {node3.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb633ea-5a62-4cdf-bf52-c6d23250c4fb",
   "metadata": {},
   "source": [
    "### 3-1. Find all objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0587ec0-5c61-4fa7-abcc-461e4302d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = tree.findall('object')\n",
    "num_objs = len(objs)\n",
    "print(\"the number of objects:\", num_objs)\n",
    "print(\"objs:\", objs) # List[Element]\n",
    "\n",
    "for obj_idx, obj in enumerate(objs):\n",
    "    for node in obj:\n",
    "        print(f\">>>> {obj_idx}th {node.tag}: {node.text}\")\n",
    "        for node2 in node:\n",
    "            print(f\">>>> {obj_idx}th bounding box's {node2.tag}: {node2.text}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba1b4b8-e764-447a-a6c7-5b2b290e2708",
   "metadata": {},
   "source": [
    "### 3-2. Obtain bounding boxes as np.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a145d0a-7369-42d4-9370-133a53c3c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = np.zeros((num_objs, 4), dtype=np.int32)\n",
    "classes = []\n",
    "for obj_idx, obj in enumerate(objs):\n",
    "    ###Get bounding boxes\n",
    "    bbox = obj.find('bndbox')\n",
    "    x1 = float(bbox.find('xmin').text) - 1 # correct coordinates\n",
    "    y1 = float(bbox.find('ymin').text) - 1\n",
    "    x2 = float(bbox.find('xmax').text) - 1\n",
    "    y2 = float(bbox.find('ymax').text) - 1\n",
    "    boxes[obj_idx, :] = [x1, y1, x2, y2]\n",
    "    \n",
    "    ###Get Categories\n",
    "    object_class = obj.find('name').text   # class name *as string\n",
    "    classes.append(object_class)\n",
    "    \n",
    "print(boxes)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43a60e-2185-4641-b3da-432ff03e9597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open(f'./VOC2007/JPEGImages/{sample_idx:06d}.jpg')\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b1910-8e77-4cc6-8164-6430530d5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bb_voc(img, boxes, classes, red_only=False):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    for box, class_ in zip(boxes, classes):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), # the upper left point\n",
    "            box[2]-box[0], # delta_x: width\n",
    "            box[3]-box[1], # delta_y: height\n",
    "            linewidth=3, # thickness\n",
    "            edgecolor='r' if red_only else VOC2007_COLORS[VOC2007_CLASSES.index(class_)],\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)\n",
    "\n",
    "draw_bb_voc(img, boxes, classes, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e14a1f-ec12-4490-961f-6a9d06f38821",
   "metadata": {},
   "source": [
    "## 4. Change into COCO format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084939bc-ca54-4474-b8e3-4be1f2d7d5e1",
   "metadata": {},
   "source": [
    "### COCO Json format example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88dfefa-6ed8-462d-a235-3f5408a2a259",
   "metadata": {},
   "source": [
    "- json is a kind of dict\n",
    "\n",
    ">root\n",
    ">>type : \"instances\", \"captions\", \"person_keypoints\"   \n",
    ">>images\n",
    ">>>file_name : image_file_name (include type)   \n",
    ">>>height : image height   \n",
    ">>>width : image width   \n",
    ">>>id : image_id (without file type)   \n",
    "\n",
    ">>annotations : a dict containing the following key\n",
    ">>>area : The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes   \n",
    ">>>iscrowd : (UInt8Tensor[N]): instances with iscrowd=True will be ignored during evaluation(all set zero)   \n",
    ">>>bbox : (FloatTensor[N, 4]) :  the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
    ">>>category_id : coco category id   \n",
    ">>>ignore : 0 / 1\n",
    ">>>segmentation : If exists segmentation id\n",
    ">>>image_id : an image identifier. It should be unique between all the images in the dataset, and is used during evaluation   \n",
    ">>>id : image_id (same as file_name)   \n",
    "\n",
    ">>categories :\n",
    ">>>supercategory : \"\"   \n",
    ">>>id :   \n",
    ">>>name :   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858e452-f451-4e3c-a626-cc56da5d5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class to class_id labels\n",
    "import os\n",
    "import pickle\n",
    "os.makedirs('./VOC2COCO/annotations', exist_ok=True)\n",
    "\n",
    "## PASCAL_VOC_LABLES\n",
    "class_list = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable','dog', 'horse', 'motorbike', 'person', 'pottedplant','sheep', 'sofa', 'train', 'tvmonitor'] \n",
    "\n",
    "class_dict_path = 'VOC2COCO/labels.txt'\n",
    "with open(class_dict_path, 'w')as f:\n",
    "    f.write('\\n'.join(class_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605a67d-6632-4763-b0b7-21eaf9241d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/yukkyo/voc2coco/blob/master/voc2coco.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "def get_label2id(labels_path: str) -> Dict[str, int]:\n",
    "    \"\"\"id is 1 start\"\"\"\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels_str = f.read().split()\n",
    "    labels_ids = list(range(1, len(labels_str)+1))\n",
    "\n",
    "    return dict(zip(labels_str, labels_ids))\n",
    "\n",
    "def get_annpaths(ann_dir_path: str = None,\n",
    "                 ann_ids_path: str = None,\n",
    "                 ext: str = '') -> List[str]:\n",
    "    # If use annotaion ids list\n",
    "    ext_with_dot = '.' + ext if ext != '' else ''\n",
    "    with open(ann_ids_path, 'r') as f:\n",
    "        ann_ids = f.read().split()\n",
    "    ann_paths = [os.path.join(ann_dir_path, aid+ext_with_dot) for aid in ann_ids]\n",
    "    return ann_paths\n",
    "\n",
    "\n",
    "def get_image_info(annotation_root):\n",
    "    \n",
    "    filename = annotation_root.findtext('filename')\n",
    "    \n",
    "    img_name = os.path.basename(filename)\n",
    "    img_id = os.path.splitext(img_name)[0]\n",
    "\n",
    "    size = annotation_root.find('size')\n",
    "    width = int(size.findtext('width'))\n",
    "    height = int(size.findtext('height'))\n",
    "\n",
    "    image_info = {\n",
    "        'file_name': filename,\n",
    "        'height': height,\n",
    "        'width': width,\n",
    "        'id': int(img_id)\n",
    "    }\n",
    "    return image_info\n",
    "\n",
    "\n",
    "def get_coco_annotation_from_obj(obj, label2id):\n",
    "    label = obj.findtext('name')\n",
    "    assert label in label2id, f\"Error: {label} is not in label2id !\"\n",
    "    category_id = label2id[label]\n",
    "    bndbox = obj.find('bndbox')\n",
    "    xmin = float(bndbox.findtext('xmin')) - 1\n",
    "    ymin = float(bndbox.findtext('ymin')) - 1\n",
    "    xmax = float(bndbox.findtext('xmax'))\n",
    "    ymax = float(bndbox.findtext('ymax'))\n",
    "    assert xmax > xmin and ymax > ymin, f\"Box size error !: (xmin, ymin, xmax, ymax): {xmin, ymin, xmax, ymax}\"\n",
    "    o_width = xmax - xmin\n",
    "    o_height = ymax - ymin\n",
    "    ann = {\n",
    "        'area': o_width * o_height,\n",
    "        'iscrowd': 0,\n",
    "        'bbox': [xmin, ymin, o_width, o_height],\n",
    "        'category_id': category_id,\n",
    "        'segmentation': []  # This script is not for segmentation\n",
    "    }\n",
    "    return ann\n",
    "\n",
    "\n",
    "def convert_xmls_to_cocojson(annotation_paths: List[str],\n",
    "                             label2id: Dict[str, int],\n",
    "                             output_jsonpath: str):\n",
    "    output_json_dict = {\n",
    "        \"images\": [],\n",
    "        \"type\": \"instances\",\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "    bnd_id = 1  # START_BOUNDING_BOX_ID\n",
    "    print('Start converting !')\n",
    "    for a_path in tqdm(annotation_paths):\n",
    "        # Read annotation xml\n",
    "        ann_tree = ET.parse(a_path)\n",
    "        ann_root = ann_tree.getroot()\n",
    "\n",
    "        img_info = get_image_info(annotation_root=ann_root)\n",
    "        img_id = img_info['id']\n",
    "        output_json_dict['images'].append(img_info)\n",
    "\n",
    "        for obj in ann_root.findall('object'):\n",
    "            ann = get_coco_annotation_from_obj(obj=obj, label2id=label2id)\n",
    "            ann.update({'image_id': img_id, 'id': bnd_id})\n",
    "            output_json_dict['annotations'].append(ann)\n",
    "            bnd_id = bnd_id + 1\n",
    "\n",
    "    for label, label_id in label2id.items():\n",
    "        category_info = {'supercategory': 'none', 'id': label_id, 'name': label}\n",
    "        output_json_dict['categories'].append(category_info)\n",
    "\n",
    "    with open(output_jsonpath, 'w') as f:\n",
    "        output_json = json.dumps(output_json_dict)\n",
    "        f.write(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa175e-2b53-41d9-9427-9594f40fb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc2coco(ann_dir, ann_ids, labels, output, ext='xml'):\n",
    "\n",
    "    label2id = get_label2id(labels_path=labels)\n",
    "    ann_paths = get_annpaths(\n",
    "        ann_dir_path=ann_dir,\n",
    "        ann_ids_path=ann_ids,\n",
    "        ext=ext,\n",
    "    )\n",
    "    \n",
    "    convert_xmls_to_cocojson(\n",
    "        annotation_paths=ann_paths,\n",
    "        label2id=label2id,\n",
    "        output_jsonpath=output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98225d62-0f04-466b-83b5-8d980a362382",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc2coco(ann_dir='VOC2007/Annotations',\n",
    "     ann_ids='VOC2007/ImageSets/Layout/train.txt',\n",
    "     labels='VOC2007/labels.txt',\n",
    "     output='VOC2COCO/annotations/instances_train2017.json')\n",
    "\n",
    "voc2coco(ann_dir='VOC2007/Annotations',\n",
    "     ann_ids='VOC2007/ImageSets/Layout/test.txt',\n",
    "     labels='VOC2007/labels.txt',\n",
    "     output='VOC2COCO/annotations/instances_test2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c6948-e141-4cd3-bfc8-194f8d1444b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "os.makedirs('./VOC2COCO/train2017', exist_ok=True)\n",
    "os.makedirs('./VOC2COCO/val2017', exist_ok=True)\n",
    "\n",
    "f = open('VOC2COCO/annotations/instances_train2017.json', 'r')\n",
    "train = json.load(f)\n",
    "for data in train['images']:\n",
    "    shutil.copy2(f\"./VOC2007/JPEGImages/{data['file_name']}\", f\"./VOC2COCO/train2017/{data['file_name']}\")\n",
    "\n",
    "f = open('VOC2COCO/annotations/instances_test2017.json', 'r')\n",
    "val = json.load(f)\n",
    "for data in val['images']:\n",
    "    shutil.copy2(f\"./VOC2007/JPEGImages/{data['file_name']}\", f\"./VOC2COCO/val2017/{data['file_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07a34d-fb11-471d-9a25-7ab0a286b9d9",
   "metadata": {},
   "source": [
    "## 4. Define CUSTOM DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0e144-7af4-451f-927b-3e0e5221aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class_list = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable','dog', 'horse', 'motorbike', 'person', 'pottedplant','sheep', 'sofa', 'train', 'tvmonitor'] \n",
    "class_dict = {}\n",
    "for i in range(len(class_list)):\n",
    "    class_dict[i+1] = class_list[i]\n",
    "\n",
    "class PASCAL_DATASET(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, image_dir, split='train'):        \n",
    "        self.data_root = data_root\n",
    "        self.image_dir = os.path.join(data_root, image_dir)\n",
    "        self.img_list = [file for file in os.listdir(self.image_dir) if file.endswith(r'.jpg')]\n",
    "        annotation_dir = os.path.join(data_root, 'annotations', f'instances_{split}2017.json')\n",
    "        self.class_dict = class_dict\n",
    "        self.boxes, self.gt_classes_str = self._load_annotation(annotation_dir)\n",
    "                                  \n",
    "    def _load_annotation(self, annotation_dir):\n",
    "        boxes = {}\n",
    "        gt_classes_str = {}\n",
    "        \n",
    "        with open(annotation_dir)as f: \n",
    "            data = json.load(f)\n",
    "        \n",
    "        for box_dict in data['annotations']:\n",
    "            bbox = box_dict['bbox']\n",
    "            category = self.class_dict[box_dict['category_id']]\n",
    "            if box_dict['image_id'] not in boxes:\n",
    "               boxes[box_dict['image_id']] = [bbox]\n",
    "               gt_classes_str[box_dict['image_id']] = [category]\n",
    "            else:\n",
    "               boxes[box_dict['image_id']] += [bbox]\n",
    "               gt_classes_str[box_dict['image_id']] += [category]\n",
    "    \n",
    "\n",
    "        return boxes, gt_classes_str\n",
    "                                      \n",
    "    def __len__(self,):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_list[index]\n",
    "        img = Image.open(os.path.join(self.image_dir, img_path))\n",
    "        img_idx = int(os.path.splitext(img_path)[0])\n",
    "        boxes, gt_classes_str = self.boxes[img_idx], self.gt_classes_str[img_idx]\n",
    "        boxes = np.array(boxes)\n",
    "        return img, boxes, gt_classes_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44fec3-fdbb-4895-aa5e-3839eae873c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_data = PASCAL_DATASET('./VOC2COCO', 'train2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc9a19-b95b-48a5-ae12-8c2cb3c59641",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_it = iter(pascal_data)\n",
    "first_data = next(pascal_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3fb73-e741-4333-8e7a-5e80e8b63692",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5e8c3-96a5-40c0-980e-bb094b8ac8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bb_coco(img, boxes, classes, red_only=False):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    for box, class_ in zip(boxes, classes):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), # the upper left point\n",
    "            box[2], # width\n",
    "            box[3], # height\n",
    "            linewidth=3, # thickness\n",
    "            edgecolor='r' if red_only else VOC2007_COLORS[VOC2007_CLASSES.index(class_)],\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7a69b-802a-434d-abb3-177d1ca5b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bb_coco(first_data[0], first_data[1], first_data[2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6e896-b18f-4bae-af49-b5e8145853b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_data = next(pascal_it)\n",
    "print(next_data)\n",
    "draw_bb_coco(next_data[0], next_data[1], next_data[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

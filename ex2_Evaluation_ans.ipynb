{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4736930-fce6-427f-a34c-39d4ea0a1388",
   "metadata": {},
   "source": [
    "# Intersection of Union & Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2d837-ccbd-4b9d-9852-b640b3a5abf9",
   "metadata": {},
   "source": [
    "## 1. Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c851122-45b6-4bdd-a88a-fc503e92a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "### Funtion for visualization\n",
    "def draw_bb(img, boxes, color='r'):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    for box in boxes:\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0],box[1]),\n",
    "            box[2],\n",
    "            box[3],\n",
    "            linewidth=3,\n",
    "            edgecolor=color,\n",
    "            facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "### Fuction for vislualize bounding with two differnt colors\n",
    "def draw_bb2(img, boxes1, boxes2, color1='r', color2='g'):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    for box in boxes1:\n",
    "        rect = patches.Rectangle((box[0],box[1]),box[2],box[3],linewidth=3,edgecolor=color1,facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    for box in boxes2:\n",
    "        rect = patches.Rectangle((box[0],box[1]),box[2],box[3],linewidth=3,edgecolor=color2,facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab28baf-6b7f-44a8-92e4-ffa3224e6d66",
   "metadata": {},
   "source": [
    "## 2. PASCAL VOC2007 Dataset in COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66df325-9bf3-4100-8216-1174a805defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class_list = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable','dog', 'horse', 'motorbike', 'person', 'pottedplant','sheep', 'sofa', 'train', 'tvmonitor'] \n",
    "class_dict = {}\n",
    "for i in range(len(class_list)):\n",
    "    class_dict[i+1] = class_list[i]\n",
    "\n",
    "class PASCAL_DATASET(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, image_dir, split='train'):        \n",
    "        self.data_root = data_root\n",
    "        self.image_dir = os.path.join(data_root, image_dir)\n",
    "        self.img_list = [file for file in os.listdir(self.image_dir) if file.endswith(r'.jpg')]\n",
    "        annotation_dir = os.path.join(data_root, 'annotations', f'instances_{split}2017.json')\n",
    "        self.class_dict = class_dict\n",
    "        self.boxes, self.gt_classes_str = self._load_annotation(annotation_dir)\n",
    "                                  \n",
    "    def _load_annotation(self, annotation_dir):\n",
    "        boxes = {}\n",
    "        gt_classes_str = {}\n",
    "        \n",
    "        with open(annotation_dir)as f: \n",
    "            data = json.load(f)\n",
    "        \n",
    "        for box_dict in data['annotations']:\n",
    "            bbox = box_dict['bbox']\n",
    "            category = self.class_dict[box_dict['category_id']]\n",
    "            if box_dict['image_id'] not in boxes:\n",
    "               boxes[box_dict['image_id']] = [bbox]\n",
    "               gt_classes_str[box_dict['image_id']] = [category]\n",
    "            else:\n",
    "               boxes[box_dict['image_id']] += [bbox]\n",
    "               gt_classes_str[box_dict['image_id']] += [category]\n",
    "    \n",
    "\n",
    "        return boxes, gt_classes_str\n",
    "                                      \n",
    "    def __len__(self,):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_list[index]\n",
    "        img = Image.open(os.path.join(self.image_dir, img_path))\n",
    "        img_idx = int(os.path.splitext(img_path)[0])\n",
    "        boxes, gt_classes_str = self.boxes[img_idx], self.gt_classes_str[img_idx]\n",
    "        boxes = np.array(boxes)\n",
    "        return img, boxes, gt_classes_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9087011-54c7-4037-918f-5a07c0c865db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_data = PASCAL_DATASET('./VOC2COCO', 'train2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3d79e-866d-4bc2-a8aa-5a64578891a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_it = iter(pascal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dcbb5-8cbc-43b5-9ea0-aa5b780b2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_boxes, gt_box_classes = next(pascal_it)\n",
    "draw_bb(img, gt_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c660f30-151d-4e93-904f-57b07fc4c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_boxes, gt_box_classes = next(pascal_it)\n",
    "draw_bb(img, gt_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0fd110-3679-4f11-9154-75a887c35393",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_boxes, gt_box_classes = next(pascal_it)\n",
    "draw_bb(img, gt_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12aa3f-ef29-442e-8a90-c69726e1a101",
   "metadata": {},
   "source": [
    "## 3. Calculate IoU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f10180-1128-4752-9f47-a440c28351d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Our ground truth box\n",
    "print(gt_boxes)\n",
    "print(gt_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0bfda-bfb2-426a-a0b7-5a99704721a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assume we have following predictions from network\n",
    "predicted_boxes = np.zeros((4, 4), dtype=np.int32)\n",
    "predicted_boxes[0, :] = [110, 80, 100, 120]\n",
    "predicted_boxes[1, :] = [50, 40, 110, 80]\n",
    "predicted_boxes[2, :] = [130, 120, 100, 180]\n",
    "predicted_boxes[3, :] = [200, 50, 150, 300]\n",
    "predicted_scores = np.array([0.9, 0.8, 0.7, 0.6])\n",
    "print(predicted_boxes)\n",
    "print(predicted_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9e640-7cce-41b6-9226-360baaf7b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bb2(img, gt_boxes, predicted_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ca9f8-74cb-47e1-9936-cda2bb2f9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to pytorch tensors\n",
    "gt_boxes = torch.from_numpy(gt_boxes).float()\n",
    "one_box = torch.unsqueeze(gt_boxes[0], 0)\n",
    "predicted_boxes = torch.from_numpy(predicted_boxes).float()\n",
    "predicted_scores = torch.from_numpy(predicted_scores).float()\n",
    "print(predicted_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66318a3-d5b0-4dee-ab52-ae4b8297c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes \n",
    "    box 1 : (1, 4) shaped pytorch tensors - sinlge GT bounding box\n",
    "    box 2 : (N, 4) shaped pytorch tensors - multiple predictions from network\n",
    "    \"\"\"\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,0]+box1[:,2], box1[:,1]+box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,0]+box2[:,2], box2[:,1]+box2[:,3]\n",
    "    \n",
    "    ## intersection rectangle coordinate\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    \n",
    "    ## practice\n",
    "    inter_area = torch.clamp(inter_rect_x2-inter_rect_x1, min=0.)\\\n",
    "            * torch.clamp(inter_rect_y2-inter_rect_y1, min=0.)\n",
    "                 \n",
    "    \n",
    "    ## calculate iou\n",
    "    area_1 = (b1_x2-b1_x1) * (b1_y2-b1_y1)\n",
    "    area_2 = (b2_x2-b2_x1) * (b2_y2-b2_y1)\n",
    "    iou = inter_area/(area_1+area_2-inter_area)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb34ca-f82d-420b-9ffb-35f2534960df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get iou score for each prediction boxes\n",
    "ious = bbox_iou(one_box, predicted_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c01fbe-bfcf-4abf-a034-c97b98f8f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ious)\n",
    "print(predicted_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191d6c2-df74-4089-8eb2-c8ff1601bb4c",
   "metadata": {},
   "source": [
    "## 4. Threshold bounding boxes based on IoU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309cf90-c151-428a-991b-459fac9769ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "th = np.where(ious.numpy() > threshold)\n",
    "th_boxes = predicted_boxes[th]\n",
    "draw_bb2(img, one_box, th_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9f9ad-05bf-4bcd-9d8c-718c6eaf90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "th = np.where(ious.numpy() > threshold)\n",
    "th_boxes = predicted_boxes[th]\n",
    "draw_bb2(img, one_box, th_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df376146-c870-4d27-af2c-5460c954b2e1",
   "metadata": {},
   "source": [
    "## 5. Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b7401-455f-473b-a3ef-4253eb0399cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class_list = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable','dog', 'horse', 'motorbike', 'person', 'pottedplant','sheep', 'sofa', 'train', 'tvmonitor'] \n",
    "\n",
    "\n",
    "def make_test_observations(pascal_it, num_image = 10):\n",
    "    images = []\n",
    "    groundtruths = []\n",
    "    detections = []\n",
    "    for i in range(num_image):\n",
    "        img, gt_boxes, gt_box_classes = next(pascal_it)\n",
    "        images.append(img)\n",
    "        for j in range(len(gt_box_classes)):\n",
    "            groundtruths.append([i, gt_box_classes[j], 1.0, (list(gt_boxes[j]))])\n",
    "        \n",
    "            rand_seed = random.random()\n",
    "            pred_cls = class_list.index(gt_box_classes[j])\n",
    "            if rand_seed < 0.33:\n",
    "                pred_cls = random.randrange(0, 21)\n",
    "            box_jitter = []\n",
    "            for k in range(4):\n",
    "                random_jitter = random.random()*2-1.0\n",
    "                jitter_scale = random.random()*10\n",
    "                box_jitter.append(random_jitter*jitter_scale)\n",
    "            box_jitter = np.asarray(box_jitter)\n",
    "            pred_boxes = box_jitter + gt_boxes[j]\n",
    "\n",
    "            detections.append([i, class_list[pred_cls], random.random(), (list(pred_boxes))])\n",
    "        \n",
    "    return images, groundtruths, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2360b-331e-4323-95f5-1cca0ed4f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, groundtruths, detections = make_test_observations(pascal_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99077306-0754-4e5a-b6a5-0676beff313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585af38-a800-47e0-82ba-58bebdb88ed8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b403428-4e75-4f58-81b0-d251f66c478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAveragePrecision(rec, prec):\n",
    "    \n",
    "    mrec = [0] + [e for e in rec] + [1]\n",
    "    mpre = [0] + [e for e in prec] + [0]\n",
    "\n",
    "    for i in range(len(mpre)-1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "\n",
    "    ii = []\n",
    "\n",
    "    for i in range(len(mrec)-1):\n",
    "        if mrec[1:][i] != mrec[0:-1][i]:\n",
    "            ii.append(i+1)\n",
    "\n",
    "    ap = 0\n",
    "    for i in ii:\n",
    "        ap = ap + np.sum((mrec[i] - mrec[i-1]) * mpre[i])\n",
    "    \n",
    "    return [ap, mpre[0:len(mpre)-1], mrec[0:len(mpre)-1], ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569ce7f-c05f-4155-9a87-b75660ebfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class_list = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable','dog', 'horse', 'motorbike', 'person', 'pottedplant','sheep', 'sofa', 'train', 'tvmonitor'] \n",
    "\n",
    "\n",
    "def AP(detections, groundtruths, classes= class_list, IOUThreshold = 0.5):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for c in classes:\n",
    "\n",
    "        dects = [d for d in detections if d[1] == c]\n",
    "        gts = [g for g in groundtruths if g[1] == c]\n",
    "\n",
    "        npos = len(gts)\n",
    "\n",
    "        dects = sorted(dects, key = lambda conf : conf[2], reverse=True)\n",
    "\n",
    "        TP = np.zeros(len(dects))\n",
    "        FP = np.zeros(len(dects))\n",
    "\n",
    "        det = Counter(cc[0] for cc in gts)\n",
    "\n",
    "        # number of  ground truth boxes per image\n",
    "        # {99 : 2, 380 : 4, ....}\n",
    "        # {99 : [0, 0], 380 : [0, 0, 0, 0], ...}\n",
    "        for key, val in det.items():\n",
    "            det[key] = np.zeros(val)\n",
    "\n",
    "        for d in range(len(dects)):\n",
    "\n",
    "\n",
    "            gt = [gt for gt in gts if gt[0] == dects[d][0]]\n",
    "\n",
    "            iouMax = 0\n",
    "\n",
    "            for j in range(len(gt)):\n",
    "                iou1 = bbox_iou(torch.unsqueeze(torch.tensor(dects[d][3]), 0), torch.unsqueeze(torch.tensor(gt[j][3]), 0))\n",
    "                if iou1 > iouMax:\n",
    "                    iouMax = iou1\n",
    "                    jmax = j\n",
    "\n",
    "            if iouMax >= IOUThreshold:\n",
    "                if det[dects[d][0]][jmax] == 0:\n",
    "                    TP[d] = 1\n",
    "                    det[dects[d][0]][jmax] = 1\n",
    "                else:\n",
    "                    FP[d] = 1\n",
    "            else:\n",
    "                FP[d] = 1\n",
    "\n",
    "        acc_FP = np.cumsum(FP)\n",
    "        acc_TP = np.cumsum(TP)\n",
    "        if npos == 0:\n",
    "            rec = acc_TP\n",
    "        else:\n",
    "            rec = acc_TP / npos\n",
    "        prec = np.divide(acc_TP, (acc_FP + acc_TP))\n",
    "\n",
    "        [ap, mpre, mrec, ii] = calculateAveragePrecision(rec, prec)\n",
    "\n",
    "\n",
    "        r = {\n",
    "            'class' : c,\n",
    "            'precision' : prec,\n",
    "            'recall' : rec,\n",
    "            'AP' : ap,\n",
    "            'total positives' : npos,\n",
    "            'total TP' : np.sum(TP),\n",
    "            'total FP' : np.sum(FP)\n",
    "        }\n",
    "\n",
    "        result.append(r)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b84f6-ad0d-43b9-bc6a-8f513d74c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = AP(detections, groundtruths)\n",
    "for dic in result:\n",
    "    print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923b092-1dc6-4d29-8c1e-40d956a3e437",
   "metadata": {},
   "source": [
    "## 6. mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668ce9f-22e3-4218-a6e4-b4d116b5592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP(result):\n",
    "    ap = 0\n",
    "    for r in result:\n",
    "        ap += r['AP']\n",
    "    mAP = ap / len(result)\n",
    "    \n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fa879-d888-4219-80ff-b1735fac58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    print(f\"{r['class']} AP : {r['AP']}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"mAP : {mAP(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe911d-2887-4061-8590-1549f2fe24be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56f25ed-802b-4cbd-975c-b3cd366e9cc3",
   "metadata": {},
   "source": [
    "# Evalutaion of Object Detection with Transformers (DETR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf451ee2-b869-4b8c-aef8-0e33b934f470",
   "metadata": {},
   "source": [
    "## 1. Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f4d62-f745-47ae-a740-712756c183ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms as T\n",
    "from utils import *\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83246e-0e42-4dd3-8edb-357adb6a4e53",
   "metadata": {},
   "source": [
    "## 2. Preliminaries for COCO dataset and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4161437-00b0-492a-abe5-e6de86c1e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO classes\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576f768-11b7-4ad2-8690-74b1787b94e7",
   "metadata": {},
   "source": [
    "## 3. Evaluation of DETR on COCO Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ed9a5-9cfb-45dc-99dd-c7f2c01df505",
   "metadata": {},
   "source": [
    "### 3-1. Load pre-trained DETR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bd4dc-11e2-4ffc-8a7a-baca59fb941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa223b9b-04ce-4f9a-aee1-322a8f43aacd",
   "metadata": {},
   "source": [
    "### 3-2. Inference validation images on the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5575781c-adae-4428-8743-132048d15b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'http://images.cocodataset.org/val2017/000000039769.jpg',\n",
    "    'http://images.cocodataset.org/val2017/000000035279.jpg',\n",
    "    'http://images.cocodataset.org/val2017/000000037740.jpg',\n",
    "]\n",
    "\n",
    "url = urls[0]\n",
    "\n",
    "# load validation images of COCO dataset\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# mean-std normalize the input images\n",
    "img = transform(im).unsqueeze(0)\n",
    "\n",
    "# propagate through the model\n",
    "outputs = model(img)\n",
    "\n",
    "# keep only predictions with 0.9+ confidence\n",
    "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "keep = probas.max(-1).values > 0.9\n",
    "\n",
    "# convert boxes from [0; 1] to image scales\n",
    "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "    \n",
    "# plot the results\n",
    "plot_results(im, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a266fe-60a6-458a-b0d0-899b92bb69af",
   "metadata": {},
   "source": [
    "### 3-3. Visualize encoder self-attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7bb48-6842-4383-b92a-289c09bc3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lists to store the outputs via up-values\n",
    "conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "\n",
    "hooks = [\n",
    "    model.backbone[-2].register_forward_hook(\n",
    "        lambda self, input, output: conv_features.append(output)\n",
    "    ),\n",
    "    model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "        lambda self, input, output: enc_attn_weights.append(output[1])\n",
    "    ),\n",
    "    model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "        lambda self, input, output: dec_attn_weights.append(output[1])\n",
    "    ),\n",
    "]\n",
    "    \n",
    "# propagate through the model\n",
    "outputs = model(img)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# don't need the list anymore\n",
    "conv_features = conv_features[0]\n",
    "enc_attn_weights = enc_attn_weights[0]\n",
    "dec_attn_weights = dec_attn_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420264d0-a00a-4fa0-b8c0-754355f105d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the CNN\n",
    "f_map = conv_features['0']\n",
    "print(\"Encoder attention:      \", enc_attn_weights[0].shape)\n",
    "print(\"Feature map:            \", f_map.tensors.shape)\n",
    "\n",
    "# get the HxW shape of the feature maps of the CNN\n",
    "shape = f_map.tensors.shape[-2:]\n",
    "\n",
    "# and reshape the self-attention to a more interpretable shape\n",
    "sattn = enc_attn_weights[0].reshape(shape + shape)\n",
    "print(\"Reshaped self-attention:\", sattn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac0da5-043e-44d2-88bd-1d889a44aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling factor for the CNN, is 32 for DETR and 16 for DETR DC5\n",
    "fact = 32\n",
    "\n",
    "# let's select 4 reference points for visualization\n",
    "idxs = [(200, 200), (280, 400), (200, 600), (440, 800),]\n",
    "\n",
    "# here we create the canvas\n",
    "fig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n",
    "\n",
    "# and we add one plot per reference point\n",
    "gs = fig.add_gridspec(2, 4)\n",
    "axs = [\n",
    "    fig.add_subplot(gs[0, 0]),\n",
    "    fig.add_subplot(gs[1, 0]),\n",
    "    fig.add_subplot(gs[0, -1]),\n",
    "    fig.add_subplot(gs[1, -1]),\n",
    "]\n",
    "\n",
    "# for each one of the reference points, let's plot the self-attention\n",
    "# for that point\n",
    "for idx_o, ax in zip(idxs, axs):\n",
    "    idx = (idx_o[0] // fact, idx_o[1] // fact)\n",
    "    ax.imshow(sattn[..., idx[0], idx[1]], cmap='cividis', interpolation='nearest')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'self-attention{idx_o}')\n",
    "\n",
    "# and now let's add the central image, with the reference points as red circles\n",
    "fcenter_ax = fig.add_subplot(gs[:, 1:-1])\n",
    "fcenter_ax.imshow(im)\n",
    "for (y, x) in idxs:\n",
    "    scale = im.height / img.shape[-2]\n",
    "    x = ((x // fact) + 0.5) * fact\n",
    "    y = ((y // fact) + 0.5) * fact\n",
    "    fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
    "    fcenter_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf5c93-008c-4a04-8478-65823c4172c8",
   "metadata": {},
   "source": [
    "### 3-4. Visualize encoder-decoder multi-head attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7db330-b968-48d0-911f-0ef17c4f26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature map shape\n",
    "h, w = conv_features['0'].tensors.shape[-2:]\n",
    "    \n",
    "fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22, 7))\n",
    "colors = COLORS * 100\n",
    "for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n",
    "    ax = ax_i[0]\n",
    "    ax.imshow(dec_attn_weights[0, idx].view(h, w))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'query id: {idx.item()}')\n",
    "    ax = ax_i[1]\n",
    "    ax.imshow(im)\n",
    "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                               fill=False, color='blue', linewidth=3))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(CLASSES[probas[idx].argmax()])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b490f-ddff-4df9-8792-7130f766e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = AttentionVisualizer(model, transform, url)\n",
    "w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5906326-4963-4744-a467-ea27429f11f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

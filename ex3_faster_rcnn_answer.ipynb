{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Object detection (faster_rcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26685,
     "status": "ok",
     "timestamp": 1636292145277,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "7NvDwivjmFOP"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.ops import misc as misc_nn_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26685,
     "status": "ok",
     "timestamp": 1636292145277,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "7NvDwivjmFOP"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "### Funtion for visualization\n",
    "def draw_bb(img, boxes, color='r', figsize=(8,8)):\n",
    "    fig,ax = plt.subplots(1, figsize=figsize)\n",
    "    for box in boxes:\n",
    "        rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=3,edgecolor=color,facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "### Fuction for vislualize boudning with two differnt colors\n",
    "def draw_bb2(img, boxes1, boxes2, color1='r', color2='g', figsize=(20,20)):\n",
    "    fig,ax = plt.subplots(1, figsize=figsize)\n",
    "    for box in boxes1:\n",
    "        rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=3,edgecolor=color1,facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    for box in boxes2:\n",
    "        rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=3,edgecolor=color2,facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrhQ8OnamFOQ"
   },
   "source": [
    "## 2. Define Pascal VOC dataloader\n",
    "\n",
    "- Pascal VOC dataset folder structure\n",
    "\n",
    "data/VOC2007/  \n",
    "\n",
    ">ImageSets/Main/  \n",
    ">>train.txt  \n",
    ">>train_small.txt(optional)  \n",
    "\n",
    ">JPEGImages/  \n",
    ">>000005.jpg  \n",
    ">>000007.jpg  \n",
    ">>000009.jpg  \n",
    ">>000012.jpg  \n",
    ">>...  \n",
    "\n",
    ">Annotations/  \n",
    ">>000005.xml  \n",
    ">>000007.xml  \n",
    ">>000009.xml  \n",
    ">>000012.xml  \n",
    ">>...  \n",
    "\n",
    "- PASCAL VOC image example : \n",
    "\n",
    "![PASCAL VOC SAMPLE IMAGE](./VOC2007/JPEGImages/000010.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsVTNpEvmFOR"
   },
   "source": [
    "### 2-1. Recap of the dataloader we define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1636292166560,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "r6NrQw4rmFOR"
   },
   "outputs": [],
   "source": [
    "class PascalDataset(data.Dataset):\n",
    "    def __init__(self, voc_root, image_set, transforms):\n",
    "        ### Data root, transform\n",
    "        self.data_root = voc_root\n",
    "        self.transforms = transforms        \n",
    "\n",
    "        ### class name to class label mapping\n",
    "        self._classes = ('__background__',\n",
    "                         'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                         'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                         'cow', 'diningtable', 'dog', 'horse',\n",
    "                         'motorbike', 'person', 'pottedplant',\n",
    "                         'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "        self._class_to_ind = dict(zip(self._classes, range(len(self._classes))))\n",
    "        \n",
    "        \n",
    "        ### Load image list\n",
    "        self.img_list = []\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')## splits_dir = 'voc_root/ImageSets/Main'\n",
    "        split_f = os.path.join(splits_dir, image_set + '.txt')## split_f = voc_root/ImageSets/Main/train.txt(or trainval.txt or val.txt)\n",
    "        self._load_img_name_list(split_f)               \n",
    "\n",
    "    def _load_img_name_list(self, img_list_path):\n",
    "        with open(img_list_path) as f:\n",
    "            self.img_list = f.read().splitlines()\n",
    "                                  \n",
    "    def _load_annotation(self, index, img_path):\n",
    "        filename = os.path.join(self.data_root, 'Annotations', img_path + '.xml') ## file_name = 'self.data_root/Annotations/image_path.xml'\n",
    "        tree = ET.parse(filename)\n",
    "        objs = tree.findall('object')\n",
    "        num_objs = len(objs) \n",
    "        \n",
    "        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n",
    "        gt_classes = np.zeros((num_objs, ), dtype=np.int64)\n",
    "\n",
    "        for ix, obj in enumerate(objs):\n",
    "            bbox = obj.find('bndbox')\n",
    "            \n",
    "            # Make pixel indexes 0-based\n",
    "            x1 = float(bbox.find('xmin').text) - 1\n",
    "            y1 = float(bbox.find('ymin').text) - 1\n",
    "            x2 = float(bbox.find('xmax').text) - 1\n",
    "            y2 = float(bbox.find('ymax').text) - 1\n",
    "            \n",
    "            cls = self._class_to_ind[obj.find('name').text.lower().strip()]\n",
    "            boxes[ix, :] = [x1, y1, x2, y2]\n",
    "            gt_classes[ix] = cls        \n",
    "\n",
    "        target = {}\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        target[\"boxes\"] = boxes ## Bounding Box Annotation을 텐서로 만듬\n",
    "        target[\"labels\"] = torch.as_tensor(gt_classes, dtype=torch.int64) ## Class Label을 텐서로 만듬\n",
    "        target[\"image_id\"] = torch.tensor([index]) ## 이미지 아이디 \n",
    "        target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) ## (ymax-ymin)*(xmax-xmin) = w*h\n",
    "        target[\"iscrowd\"] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        return target\n",
    "                                      \n",
    "    def __len__(self,):## Essential Function in Custom Dataset\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):## Essential Function in Custom Datset\n",
    "        ### Load images\n",
    "        img_path = self.img_list[index]\n",
    "        img = Image.open(os.path.join(self.data_root,'JPEGImages',img_path + '.jpg')).convert(\"RGB\")\n",
    "        \n",
    "        ### Load label dictionary\n",
    "        target = self._load_annotation(index, img_path)\n",
    "\n",
    "        ### Transform\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img), target\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoLtsMilmFOS"
   },
   "source": [
    "### 2-2. Visualize a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "error",
     "timestamp": 1636292208795,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "PvbVJuUsmFOS",
    "outputId": "2356bda7-edaa-4c79-db4d-5c14df26b535"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transfroms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "voc_data = PascalDataset(voc_root='./VOC2007/', image_set='train', transforms=transfroms)\n",
    "voc_iter = iter(voc_data)\n",
    "img, label = next(voc_iter)\n",
    "\n",
    "print(img.shape, '\\n', label)\n",
    "\n",
    "img_PIL = transforms.ToPILImage()(img.clone())\n",
    "bb_label = label['boxes']\n",
    "draw_bb(img=img_PIL, boxes=bb_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cA6mKYjmFOT"
   },
   "source": [
    "## 3. Define Faster-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wan9R5WJmFOT"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# doc: https://pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO train 2017, COCO는 class 91개\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, pretrained_backbone=True)\n",
    "print(model.roi_heads.box_predictor.cls_score)\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 21  #  class (20) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "print(model.roi_heads.box_predictor.cls_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.rpn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnnIzhFlmFOT"
   },
   "source": [
    "## 4. Fine-tune Faster-RCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udoNKD5wmFOT"
   },
   "outputs": [],
   "source": [
    "transfroms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #normalization value\n",
    "])\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "voc_train = PascalDataset(voc_root='./VOC2007/', image_set='train', transforms=transfroms)\n",
    "voc_test = PascalDataset(voc_root='./VOC2007/', image_set='val', transforms=transfroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udoNKD5wmFOT"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    zip_batch = zip(*batch)\n",
    "    tuple_batch = tuple(zip_batch)\n",
    "    return tuple_batch\n",
    "\n",
    "\n",
    "### define training and validation data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    voc_train, batch_size=16, shuffle=True, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    voc_test, batch_size=16, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [['a', 'b', 'c'], [1, 2, 3]]\n",
    "tuple(zip(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udoNKD5wmFOT"
   },
   "outputs": [],
   "source": [
    "# move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66-QNA6WmFOT",
    "outputId": "65357346-9ea9-4387-ae20-86f1a2752869",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's train it for 1 epochs\n",
    "num_epochs = 30\n",
    "# print loss every 10 mini-batches\n",
    "print_step = 10\n",
    "\n",
    "# train for one epoch, printing every 10 iterations\n",
    "for epoch in range(num_epochs):\n",
    "    ## enumerate through data_loader\n",
    "    ### Train mode\n",
    "    model.train()\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader_train):\n",
    "        \n",
    "        ### Load input data\n",
    "        images = list(image.to(device) for image in images)        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        ### Forward & loss aggregation\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        ### 로스 종류 확인\n",
    "        ### print(loss_dict.keys())\n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "    \n",
    "        ### BackPropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### print itermedicate outputs\n",
    "        if(batch_idx % print_step == 0):\n",
    "            print('Epoch: [{}/{}], Iterations: [{}/{}], loss: {}  '.format(epoch, num_epochs, batch_idx, len(data_loader_train), loss_value))\n",
    "\n",
    "print('Training Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE2lUJKXmFOU",
    "outputId": "94e0137c-7959-4569-f75d-33e075db9d87"
   },
   "outputs": [],
   "source": [
    "# predefined evaluation function\n",
    "from engine import evaluate\n",
    "#egg=pycocotools^&subdirectory=PythonAPI.\n",
    "evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF5Z6LRAmFOU"
   },
   "source": [
    "## 5. Visulize the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUUhO1A3mFOU"
   },
   "outputs": [],
   "source": [
    "# eval mode\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NgU4G8PmFOU"
   },
   "outputs": [],
   "source": [
    "data_it = iter(data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NgU4G8PmFOU"
   },
   "source": [
    "'__background__' - 0,\n",
    "'aeroplane' - 1,\n",
    "'bicycle' - 2,\n",
    "'bird' - 3,\n",
    "'boat'- 4,\n",
    "'bottle'- 5,\n",
    "'bus'- 6,\n",
    "'car'- 7,\n",
    "'cat'- 8,\n",
    "'chair'- 9,\n",
    "'cow'- 10,\n",
    "'diningtable' - 11,\n",
    "'dog' - 12,\n",
    "'horse' - 13,\n",
    "'motorbike' - 14,\n",
    "'person' - 15,\n",
    "'pottedplant' - 16,\n",
    "'sheep' - 17,\n",
    "'sofa'- 18,\n",
    "'train' - 19,\n",
    "'tvmonitor' - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi8SvxKemFOU",
    "outputId": "3f21de92-2211-43ed-9485-6aef9498f751"
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(data_it)\n",
    "\n",
    "img_test = imgs[0]\n",
    "bbox_test = labels[0]['boxes']\n",
    "\n",
    "prediction = model(list([img_test.to(device)]))\n",
    "scores = prediction[0]['scores']\n",
    "print(scores)\n",
    "threshold = (scores>0.8).sum().cpu().detach().item()\n",
    "print(threshold)\n",
    "### transform for visulization\n",
    "P = torchvision.transforms.ToPILImage()\n",
    "img_test_PIL = P(img_test)\n",
    "prediction_bbox_np = prediction[0]['boxes'].cpu().detach().numpy()[:threshold]\n",
    "draw_bb2(img_test_PIL, bbox_test, prediction_bbox_np)\n",
    "print(prediction[0]['labels'])\n",
    "print(prediction_bbox_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haQah0EPmFOU"
   },
   "source": [
    "## (Optional) Test on an image you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbHkworDmFOV",
    "outputId": "047d5c0c-fce9-4c72-e973-3cf3dd7b83d2"
   },
   "outputs": [],
   "source": [
    "# read image\n",
    "img_demo = Image.open('./VOC2007/JPEGImages/000010.jpg').convert(\"RGB\")\n",
    "img_demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbHkworDmFOV",
    "outputId": "047d5c0c-fce9-4c72-e973-3cf3dd7b83d2"
   },
   "source": [
    "'__background__' - 0,\n",
    "'aeroplane' - 1,\n",
    "'bicycle' - 2,\n",
    "'bird' - 3,\n",
    "'boat'- 4,\n",
    "'bottle'- 5,\n",
    "'bus'- 6,\n",
    "'car'- 7,\n",
    "'cat'- 8,\n",
    "'chair'- 9,\n",
    "'cow'- 10,\n",
    "'diningtable' - 11,\n",
    "'dog' - 12,\n",
    "'horse' - 13,\n",
    "'motorbike' - 14,\n",
    "'person' - 15,\n",
    "'pottedplant' - 16,\n",
    "'sheep' - 17,\n",
    "'sofa'- 18,\n",
    "'train' - 19,\n",
    "'tvmonitor' - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LeJr6BRmFOV",
    "outputId": "fad5b3a4-f46a-446a-e43a-5427dcef6dba"
   },
   "outputs": [],
   "source": [
    "# Forward through trained model\n",
    "bboxes_demo = model([torchvision.transforms.ToTensor()(img_demo).to(device)])[0]['boxes'].cpu().detach()\n",
    "scores_demo = model([torchvision.transforms.ToTensor()(img_demo).to(device)])[0]['scores'].cpu().detach()\n",
    "labels_demo = model([torchvision.transforms.ToTensor()(img_demo).to(device)])[0]['labels'].cpu().detach()\n",
    "\n",
    "threshold = (scores_demo>0.1).sum().item()\n",
    "\n",
    "print(scores_demo)\n",
    "print(scores_demo[:threshold])\n",
    "print(labels_demo[:threshold])\n",
    "print(bboxes_demo[:threshold])\n",
    "\n",
    "# Draw bounding boxes\n",
    "draw_bb(img_demo, bboxes_demo[:threshold], 'r', (12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, threshold):\n",
    "    \"\"\"\n",
    "    boxes: (N, 4), each row-> (x1, y1, x2, y2), x2 > x1, y2 > y1\n",
    "    scores: (N,), each value in [0, 1]\n",
    "    threshold: iou threshold\n",
    "    \"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    \n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()\n",
    "    \n",
    "    keep = []\n",
    "    \n",
    "    while len(order) > 0:\n",
    "        # the index with the highest score.\n",
    "        idx = order[-1]\n",
    "        # keep it.\n",
    "        keep.append(boxes[idx])\n",
    "        # remove the index from the candidate list.\n",
    "        order = order[:-1]\n",
    "        \n",
    "        if len(order) == 0:\n",
    "            break # check\n",
    "        \n",
    "        # order bounding boxes according to the order.\n",
    "        xx1 = x1[order]\n",
    "        xx2 = x2[order]\n",
    "        yy1 = y1[order]\n",
    "        yy2 = y2[order]\n",
    "        \n",
    "        # intersection coordinate\n",
    "        ix1 = torch.max(xx1, boxes[idx, 0])\n",
    "        iy1 = torch.max(yy1, boxes[idx, 1])\n",
    "        ix2 = torch.min(xx2, boxes[idx, 2])\n",
    "        iy2 = torch.min(yy2, boxes[idx, 3])\n",
    "        \n",
    "        # width and height of the intesection, calculate intersection\n",
    "        w = torch.clamp(ix2-ix1, min=0.)\n",
    "        h = torch.clamp(iy2-iy1, min=0.)\n",
    "        \n",
    "        inter = w*h\n",
    "        rem_areas = areas[order] + areas[idx] - 2*inter\n",
    "        union = rem_areas + inter\n",
    "        iou = inter / union\n",
    "        \n",
    "        # keep the boxes with iou less than threshold\n",
    "        order = order[iou<threshold]\n",
    "        \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "                         'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                         'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                         'cow', 'diningtable', 'dog', 'horse',\n",
    "                         'motorbike', 'person', 'pottedplant',\n",
    "                         'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "indexes_to_classes = dict(zip(range(len(classes)), classes))\n",
    "nms_boxes = nms(bboxes_demo[:threshold], scores_demo[:threshold], 0.3)\n",
    "print(len(nms_boxes))\n",
    "draw_bb(img_demo, nms_boxes, 'r', (12, 12))\n",
    "print(scores_demo[:len(nms_boxes)])\n",
    "for i in labels_demo[:len(nms_boxes)]:\n",
    "    print(indexes_to_classes[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bb(img_demo, bboxes_demo_np[:threshold], 'r', (12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Object detection tutorial simple pascal_voc data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
